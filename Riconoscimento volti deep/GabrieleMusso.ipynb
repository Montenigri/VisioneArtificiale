{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viene richiesto di costruire un sistema di riconoscimento dei volti a mondo chiuso (ovvero i nomi associati ai volti è conosciuto e definito in numero finito) attraverso un sistema di rilevamento dei volti ed uno di classificazione, il tutto applicato ad un dataset generato ad hoc.\n",
    "\n",
    "L'impementazione è stata eseguita tramite un modello pretrained fornito da Ultralytics chiamato YOLOv8 con un modello che presenta [3,2M di parametri](https://github.com/ultralytics/ultralytics#:~:text=0.99-,3.2,-8.7), anche se è il modello più piccolo mantiene comunque una [ottima precisione](https://github.com/derronqi/yolov8-face#:~:text=92.2-,79.0,-%2D). \n",
    "Ai volti rilevati vengono applicate delle modifiche come rotazione, zoom o flip orizzontale per aumentare il numero di dati disponibili alla rete.\n",
    "La classificazione è affidata ad una rete custom convoluzionale. \n",
    "\n",
    "\n",
    "Nella cartella train si trova il dataset diviso in 5 cartelle il cui nome è l'etichetta delle foto al loro interno, è presente il file requirements qualora si volesse eseguire il codice in un nuovo ambiente, il file h5 contentente i pesi del modello (Attenzione, contiene solo i pesi, sarà necessario generare il modello prima).\n",
    "I file .pkl contengono le matrici serializzate dei volti dei tre set in modo da non dover ricaricare ogni volta i file e cercare i volti (la ricerca su cpu con 4 core impiega circa 200/250ms per volto, che diventano circa 100/120 minuti per le circa 30000 facce del dataset).\n",
    "Il file .pt invece contiene i pesi per yolov8 con specifica attenzione al rilevamento dei volti "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dense, MaxPool2D, Dropout,RandomBrightness,RandomRotation,RandomFlip,RandomZoom,Resizing\n",
    "from keras.utils import to_categorical\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from math import floor\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLO utilizza una rete convoluzionale ed effettua il rilevamento degli oggetti e la loro classificazione \"guardando una volta sola\" ovvero senza avere la necessità di usare due reti dove la prima propone delle aree e la seconda riconosce gli oggetti.\n",
    "è una rete estremamente veloce utilizzabile anche in tempo reale, queste caratteristiche l'hanno portata ad essere una rete estremamente utilizzata e nel tempo migliorata, date le 8 versioni in 7 anni.\n",
    "\n",
    "L'implementazione proposta utilizza la versione sviluppata da Ultralytics con i pesi per i volti.\n",
    "\n",
    "\n",
    "Viene creato un array con i nomi in modo da poter utilizzare degli interi come label all'interno delle reti\n",
    "\n",
    "_resizer_ utilizza un layer di keras per fare il ridimensionamento dei volti rilevati mantenendo l'aspect ratio e non applicare quindi warp non voluti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/derronqi/yolov8-face\n",
    "#https://docs.ultralytics.com/\n",
    "detect = YOLO('yolov8n-face.pt')\n",
    "\n",
    "nomi = [\"Davide\",\"Francesco\", \"Gabriele\", \"Stefano\", \"Unknown\"]\n",
    "\n",
    "resizer = Resizing(\n",
    "    64,\n",
    "    64,\n",
    "    interpolation='bilinear',\n",
    "    crop_to_aspect_ratio=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data la struttura descritta in precedenza, vengono caricate le foto e le etichette con un ciclo for che cicla le 5 cartelle e salva le foto ed i relativi tag, questi vengono \"incatenati\" alle liste precedenti in quanto altrimenti avremmo una struttura annidata, con il metodo chain di itertools invece ci viene restituita una lista in cui sono presenti i vecchi elementi ed i nuovi (ex. chain delle liste [1,2,3,4] e [5,6,7], con il metodo append avremmo una lista di liste [[1,2,3,4],[5,6,7]] mentre con il metodo chain otteniamo [1,2,3,4,5,6,7]).\n",
    "\n",
    "Vengono quindi cambiati i nomi in indici in quanto le funzioni di keras si aspettano dei valori interi per etichette, questo viene fatto attravero un doppio ciclo for che scorre per ogni tag tutti i nomi e sostituisce qualora trovasse una corrispondenza.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset(root=\"train\"):\n",
    "\n",
    "    listDir = os.listdir(root)\n",
    "    foto = []\n",
    "    tag = []\n",
    "\n",
    "    for dir in listDir:\n",
    "        imgs =  glob.glob(f\"{root}/{dir}/*.jpg\")\n",
    "        dirs = [dir]*len(imgs)\n",
    "        foto = list(chain(foto,imgs))\n",
    "        tag = list(chain(tag,dirs))\n",
    "        \n",
    "\n",
    "\n",
    "    for k in tqdm(range(len(tag)), desc= \"Changing names to index\"):\n",
    "        for i in range(len(nomi)):\n",
    "            if tag[k] == nomi[i]:\n",
    "                tag[k] = i\n",
    "\n",
    "    tag = list(map(int, tag))\n",
    "    foto,tag = shuffle(foto,tag, random_state=42)\n",
    "    return foto,tag\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione _findFaces_ prende in input un frame ed il massimo di volti riconoscibili per quel frame e ritorna la lista di punti per le bbox ed i volti ritagliati, per farlo utilizza il predict della rete YOLO implementata in precedenz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFaces(frame, maxDet = 10):\n",
    "    img_test = detect.predict(source=frame,max_det=maxDet,verbose=False)\n",
    "    faces = []\n",
    "    boxesDetect = []\n",
    "    for result in img_test:\n",
    "        boxes = result.boxes  \n",
    "        boxes = boxes.numpy()\n",
    "        try:\n",
    "            face = frame[int(boxes.xyxy[0][1]):int(boxes.xyxy[0][3]),int(boxes.xyxy[0][0]):int(boxes.xyxy[0][2]),:]\n",
    "            face = resizer(face)\n",
    "        except:\n",
    "            face = np.zeros((64,64,3))\n",
    "        faces = list(chain(faces,face))\n",
    "        boxesDetect = list(chain(boxesDetect,boxes))\n",
    "    \n",
    "    return faces, boxesDetect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il dataset viene quindi diviso attraverso le percentuali definite dal parametro _percentage_ che di default vuole il 60% del set per il training, il 20% per il validation ed il 20% per il test.\n",
    "\n",
    "Vengono a questo punto anche sostituiti i path con le relative foto in cui vengono ritagliati i volti presenti (nel caso del training viene limitata questa possibilità ad un rilevamento per foto per evitare problemi qualora dovessero esserci discrepanze tra la lunghezza dei dati e delle label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSets(x,y, percentage=[0.6,0.2]):\n",
    "    length = len(x)\n",
    "    \n",
    "    trainLen = floor(percentage[0]*length)\n",
    "    valLen = floor(percentage[1]*length) + trainLen\n",
    "\n",
    "    for i in tqdm(range(len(x)), desc= \"Detecting faces\"):\n",
    "        x[i],_ = findFaces(cv2.imread(x[i]),maxDet=1)\n",
    "\n",
    "    x = list(map(np.array, x))\n",
    "    \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    train = (x[:trainLen],y[:trainLen])\n",
    "    val = (x[trainLen:valLen],y[trainLen:valLen]) \n",
    "    test  = (x[valLen:],y[valLen:])\n",
    "    return train, val, test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per rendere la rete più resistente a variabili come inclinazione del volto o variazioni di luce utilizziamo tecniche di data augmentation che prevedono modifica dei dati in ingresso.\n",
    "\n",
    "I dati dovrebbero essere quindi sostituiti per ottenere un dataset vario, in questa istanza ho deciso di includere sia i dati base che quelli aumentati sia per aumentare i dati di training, sia per non lasciare influenzare tutto l'allenamento da soli parametri randomizzati.\n",
    "\n",
    "Viene quindi usato un insieme di layer di keras data la facilità di implementazione e la sicura compatibilità con il resto della rete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  RandomFlip(\"horizontal\"),\n",
    "  RandomRotation(0.2),\n",
    "  RandomBrightness((-0.2,0.2)),\n",
    "  RandomZoom(.1, .1)\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vengono quindi cercati i dataset e vengono caricati o calcolati qualora sia la prima esecuzione o si voglia effettuare un nuovo training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"train.pkl\") and os.path.exists(\"val.pkl\") and os.path.exists(\"test.pkl\"):\n",
    "    with open(\"train.pkl\",\"rb\") as ds:\n",
    "        train = pickle.load(ds)\n",
    "    with open(\"val.pkl\",\"rb\") as ds:\n",
    "        val = pickle.load(ds)\n",
    "    with open(\"test.pkl\",\"rb\") as ds:\n",
    "        test = pickle.load(ds)\n",
    "\n",
    "else: \n",
    "    x,y = getDataset()\n",
    "\n",
    "    train, val, test = getSets(x,y)\n",
    "\n",
    "    with open(\"train.pkl\",\"wb\") as ds:\n",
    "        (X_train, Y_train) = train\n",
    "        Xtrain = []\n",
    "        Ytrain = []\n",
    "        for i in tqdm(range(len(X_train)), desc= \"data augmentation\"):\n",
    "                Xtrain.append(data_augmentation(X_train[i]))\n",
    "                Xtrain.append(X_train[i])\n",
    "                Ytrain.append(Y_train[i])\n",
    "                Ytrain.append(Y_train[i])\n",
    "\n",
    "        X_train = np.array(Xtrain)\n",
    "        Y_train = np.array(Ytrain)\n",
    "        train = (X_train,Y_train)\n",
    "        pickle.dump(train,ds)\n",
    "\n",
    "    with open(\"val.pkl\",\"wb\") as ds:\n",
    "        pickle.dump(val,ds)\n",
    "\n",
    "    with open(\"test.pkl\",\"wb\") as ds:\n",
    "        pickle.dump(test,ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le tuple vengono quindi divise tra dati e label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train) = train\n",
    "(X_val,Y_val) = val\n",
    "(X_test, Y_test) = test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il classificatore è una rete che, attraverso diversi layer, arriva a determinare la classe di un oggetto che gli viene sottoposto, nell'implementazione proposta questo viene svolto da una rete convoluzionale di profondità cinque con diverso numero di filtri e dimensione di finestra intervallati da maxpooling e dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), activation='relu', input_shape=(64, 64, 3)))\n",
    "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"Adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[keras.metrics.CategoricalAccuracy()]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se non si trovasse il file partirebbe il fit della rete con il relativo callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"pesiClassificatore.h5\"):\n",
    "    model.load_weights('pesiClassificatore.h5')\n",
    "\n",
    "else:\n",
    "    callback = keras.callbacks.EarlyStopping(monitor= \"val_loss\", patience=3,restore_best_weights=True)\n",
    "    model.fit(\n",
    "        X_train, to_categorical(Y_train), epochs=100, \n",
    "        batch_size=64, shuffle=True, \n",
    "        validation_data=(X_val,to_categorical(Y_val)),\n",
    "        callbacks=callback\n",
    "        )\n",
    "\n",
    "    model.save_weights('pesiClassificatore.h5')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il metodo evaluate ci permette di verificare come è andato l'allenamento utilizzando l'ultima porzione di dati per verificare quanto la rete prevede correttamente su dati mai visti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(\n",
    "  X_test,\n",
    "  to_categorical(Y_test)\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzione per classificare i frame, come lo faccio, cosa faccio e cosa ritorno\n",
    "\n",
    "Per classificare i volti presenti in ogni frame viene invocata la funzione _classificatore_ che utilizza nuovamente il metodo predict di YOLO per ottenere una lista dei singoli volti da passare dopo al classificatore definito precedentemente. \n",
    "\n",
    "Una volta raccolte le bbox e le label corrispondenti vengono modificati i frame originali disegnando le bbox e scrivendo le label.\n",
    "\n",
    "Viene quindi ritornato l'array di frame per essere salvato \n",
    "\n",
    "\n",
    "Nota: la documentazione suggerisce di usare il modello come funzione (model(dato)) nel caso in cui sia solo uno il valore da predirre, mentre usare il metodo predict nel caso in cui ci sia una batch di dati da predirre "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificatore(frames):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    for f in tqdm(range(len(frames)), desc=\"Face recognition per frame\"):\n",
    "        faces,boxes = findFaces(frames[f])\n",
    "        predict=model(faces)\n",
    "        for (boxe,pred) in zip(boxes, predict):\n",
    "            frames[f] = cv2.putText(frames[f], nomi[np.argmax(pred)] , (int(boxe.xyxy[0][0])-5,int(boxe.xyxy[0][1])-5),font, 1,(255,255,255),2)\n",
    "            frames[f] = cv2.rectangle(frames[f], (int(boxe.xyxy[0][0]), int(boxe.xyxy[0][1])), (int(boxe.xyxy[0][2]), int(boxe.xyxy[0][3])), (255, 0, 255), 4)\n",
    "    return frames\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viene caricato il video e smembrato in frames con 3 canali (BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture(\"Video finale senza riconoscimento.mp4\")\n",
    "frames = []\n",
    "if (video.isOpened() == False):\n",
    "    print(\"Error opening video file\")\n",
    "while(video.isOpened()):\n",
    "  ret, frame = video.read()\n",
    "  if ret == True:\n",
    "        frames.append(frame)\n",
    "  else:\n",
    "      break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raccogliamo quindi i frame classificati (Vengono tagliati ai primi 1500 frames per limiti di upload di github)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = classificatore(frames)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto vengono raccolti altezza e larghezza del singolo frame e si prepara dinamicamente il salvataggio dei frame modificati in video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width, channels = results[0].shape\n",
    "size = (width,height)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "out15 = cv2.VideoWriter('project_video_finale.mp4',fourcc, 15, size)\n",
    "\n",
    "for i in tqdm(range(len(results)), desc=\"Saving frames into video\"):\n",
    "    out15.write(results[i])\n",
    "out15.release()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel caso si volesse provare in tempo reale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classificatoreIRT(frame):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    faces,boxes = findFaces(frame)    \n",
    "    predict=model(faces)\n",
    "    try:\n",
    "        for (boxe,pred) in zip(boxes, predict):\n",
    "            frame = cv2.putText(frame, nomi[np.argmax(pred)] , (int(boxe.xyxy[0][0])-5,int(boxe.xyxy[0][1])-5),font, 1,(255,255,255),2)\n",
    "            frame = cv2.rectangle(frame, (int(boxe.xyxy[0][0]), int(boxe.xyxy[0][1])), (int(boxe.xyxy[0][2]), int(boxe.xyxy[0][3])), (255, 0, 255), 4)\n",
    "    except:\n",
    "        pass    \n",
    "    return frame\n",
    "\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "if not camera.isOpened:\n",
    "    print('--(!)Error opening video capture')\n",
    "    exit(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = camera.read()\n",
    "    if frame is None:\n",
    "        print('--(!) No captured frame -- Break!')\n",
    "        break\n",
    "    frame = classificatoreIRT(frame)\n",
    "    cv2.imshow('Capture - Face detection', frame)\n",
    "    if cv2.waitKey(10) == 27:\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sono state testate diverse reti convoluzionali la cui topografia e risultati sono riportati di seguito:\n",
    "<details><summary>Reti*</summary>\n",
    "    <details><summary>Rete0</summary>\n",
    "    Conv2D(32, (5, 5), activation='relu', input_shape=(64, 64, 3))<br>   \n",
    "    Conv2D(64, (5, 5), activation='relu')<br>\n",
    "    MaxPool2D(pool_size=(2,2))<br>\n",
    "    Dropout(0.1)<br>\n",
    "    Conv2D(64, (3, 3), activation='relu')<br>   \n",
    "    Conv2D(64, (3, 3), activation='relu')<br>\n",
    "    MaxPool2D(pool_size=(2,2))<br>\n",
    "    Conv2D(64, (3, 3), activation='relu')<br>   \n",
    "    Dropout(0.1)<br>\n",
    "    Flatten()<br>\n",
    "    Dense(5, activation='softmax')<br> \n",
    "    </details>\n",
    "    <details><summary>Rete1</summary>\n",
    "    Conv2D(32, (5, 5), activation='relu', input_shape=(64, 64, 3))<br>\n",
    "    Conv2D(64, (5, 5), activation='relu')<br>\n",
    "    MaxPool2D(pool_size=(2,2))<br>\n",
    "    Conv2D(64, (3, 3), activation='relu')<br>\n",
    "    Conv2D(64, (3, 3), activation='relu')<br>\n",
    "    MaxPool2D(pool_size=(2,2))<br>\n",
    "    Conv2D(64, (3, 3), activation='relu')<br>\n",
    "    Flatten()<br>\n",
    "    Dense(5, activation='softmax')<br>\n",
    "    </details>\n",
    "    <details><summary>Rete2</summary>\n",
    "    Conv2D(32, (5, 5), activation='relu', input_shape=(64, 64, 3))<br>\n",
    "    MaxPool2D(pool_size=(2,2))<br>\n",
    "    Conv2D(64, (3, 3), activation='relu')<br>\n",
    "    MaxPool2D(pool_size=(2,2))<br>\n",
    "    Conv2D(64, (3, 3), activation='relu')<br>\n",
    "    Flatten()<br>\n",
    "    Dense(5, activation='softmax')<br>\n",
    "    </details>\n",
    "    <details>\n",
    "    <summary>Rete3</summary>\n",
    "    Conv2D(32, (5, 5), activation='relu', input_shape=(64, 64, 3))<br>\n",
    "    Conv2D(64, (3, 3), activation='relu')<br>\n",
    "    Conv2D(64, (3, 3), activation='relu')<br>\n",
    "    Flatten()<br>\n",
    "    Dense(5, activation='softmax')<br>\n",
    "    </details>\n",
    "    <details>\n",
    "    <summary>Rete4</summary>\n",
    "    Conv2D(32, (5, 5), activation='relu', input_shape=(64, 64, 3))<br>\n",
    "    Conv2D(64, (5, 5), activation='relu')<br>\n",
    "    MaxPool2D(pool_size=(2,2))<br>\n",
    "    Dropout(0.1)<br>\n",
    "    Conv2D(64, (3, 3), activation='relu')<br>\n",
    "    Conv2D(64, (3, 3), activation='relu')<br>\n",
    "    MaxPool2D(pool_size=(2,2))<br>\n",
    "    Conv2D(64, (3, 3), activation='relu')<br>\n",
    "    Dropout(0.1)<br>\n",
    "    Conv2D(64, (3, 3), activation='relu')<br>\n",
    "    Conv2D(64, (3, 3), activation='relu')<br>\n",
    "    Flatten()<br>\n",
    "    Dense(5, activation='softmax')<br>\n",
    "    </details>\n",
    "</details> \n",
    "\n",
    "\n",
    "Le reti proposte hanno dato i seguenti risultati\n",
    "\n",
    "| #Rete | Loss** | Metrics** | # Layer Convoluzionali | # Layer Dropout | # Layer Maxpool |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| 0   | 0.1490 | 0.9452 | 5 | 2 | 2 |\n",
    "| 1   | 0.4058 | 0.8623 | 5 | 0 | 2 |\n",
    "| 2   | 0.0914 | 0.9647 | 3 | 0 | 2 |\n",
    "| 3   | 0.2000 | 0.9349 | 3 | 0 | 0 |\n",
    "| 4   | 0.0910 | 0.9622 | 7 | 2 | 2 |\n",
    "\n",
    "Il modello di loss utilizzato è il \"categorical_crossentropy\", mentre la metrica è fornita dal modulo metrics di keras ed è la CategoricalAccuracy\n",
    "\n",
    "Come è possibile notare dai valori della loss e della metrica, abbiamo risultati molto simili tra la rete 2 e la rete 4, pertanto, dato il minore numero di strati della rete 2 questa verrà preferita tra tutte\n",
    "\n",
    "\n",
    "\n",
    "*Le reti sono cosi riprodotte per facilitarne l'uso in futuro in quanto basterà copiare le istruzioni direttamente\n",
    "**Valore troncato al quarto decimale"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La versione più recente di questo progetto è disponibile [qui](https://github.com/Montenigri/VisioneArtificiale/tree/main/Riconoscimento%20volti%20deep)\n",
    "\n",
    "Fonti:\n",
    "\n",
    "[Benchmark](https://github.com/derronqi/yolov8-face)\n",
    "\n",
    "[Numero parametri](https://github.com/ultralytics/ultralytics)\n",
    "\n",
    "[YOLO](https://12ft.io/proxy?&q=https%3A%2F%2Ftowardsdatascience.com%2Fyolo-you-only-look-once-real-time-object-detection-explained-492dc9230006)\n",
    "\n",
    "[Predict vs Funzione](https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
