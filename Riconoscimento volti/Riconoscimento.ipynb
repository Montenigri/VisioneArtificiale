{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per il riconoscimento facciale abbiamo bisogno di seguire i passi indicati nel paper firmato da Turk e Pentland:\n",
    "- Aquisire un dataset di facce\n",
    "- Calcolare le eigenfaces dal set di apprendimento conservando solo le M immagini che hanno gli autovalori più alti. Queste M facce rappresentano lo spazio delle facce, questo spazio può essere man mano aggiornato\n",
    "- Calcolare la distribuzione M dimensionale per ogni individuo proiettando la faccia sullo spazio delle facce\n",
    "\n",
    "Una volta inizializzato il sistema, possiamo procedere in questo modo:\n",
    "- Calcolare un set di pesi in base all'immagine fornita e gli autovalori proiettando l'immagine su ogni eigenfaces \n",
    "- Determinare se l'immagine è effettivamente un volto controllando che sia abbastanza vicino allo spazio delle facce\n",
    "- Calcolare i pesi in modo da definire se è una persona che è già segnata o meno\n",
    "- Opzionalmente si possono aggiornare pesi e spazio delle facce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob \n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import math\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 64\n",
    "\n",
    "faces_xml = 'haarcascade_frontalface_alt.xml'\n",
    "#eyes_xml = 'haarcascade_eye.xml'\n",
    "face_cascade = cv2.CascadeClassifier()\n",
    "#eyes_cascade = cv2.CascadeClassifier()\n",
    "if not face_cascade.load(faces_xml):\n",
    "    print('--(!)Errore durante il caricamento del file xml per le facce')\n",
    "    exit(0)\n",
    "\n",
    "\n",
    "#if not eyes_cascade.load(eyes_xml):\n",
    "#    print('--(!)Error loading eyes cascade')\n",
    "#    exit(0)\n",
    "\n",
    "root = \"foto64x64\"\n",
    "cwd = os.getcwd()\n",
    "listDir = os.listdir(root)\n",
    "tagFoto = {}\n",
    "\n",
    "for dir in listDir:\n",
    "    imgs =  glob.glob(f\"{root}/{dir}/*.jpg\")\n",
    "    tagFoto[dir] = imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def preprocess(face):\n",
    "    new_img = face\n",
    "    img_raw = face\n",
    "    eyes = eyes_cascade.detectMultiScale(face)\n",
    "    if len(eyes) >= 2:\n",
    "        eye = eyes[:, 2]\n",
    "        container1 = []\n",
    "        for i in range(0, len(eye)):\n",
    "            container = (eye[i], i)\n",
    "            container1.append(container)\n",
    "        df = pd.DataFrame(container1, columns=[\"length\", \"idx\"]).sort_values(by=['length'])\n",
    "        eyes = eyes[df.idx.values[0:2]]\n",
    " \n",
    "        # deciding to choose left and right eye\n",
    "        eye_1 = eyes[0]\n",
    "        eye_2 = eyes[1]\n",
    "        if eye_1[0] > eye_2[0]:\n",
    "            left_eye = eye_2\n",
    "            right_eye = eye_1\n",
    "        else:\n",
    "            left_eye = eye_1\n",
    "            right_eye = eye_2\n",
    " \n",
    "        # center of eyes\n",
    "        # center of right eye\n",
    "        right_eye_center = (\n",
    "            int(right_eye[0] + (right_eye[2]/2)),\n",
    "          int(right_eye[1] + (right_eye[3]/2)))\n",
    "        right_eye_x = right_eye_center[0]\n",
    "        right_eye_y = right_eye_center[1]\n",
    " \n",
    "        # center of left eye\n",
    "        left_eye_center = (\n",
    "            int(left_eye[0] + (left_eye[2] / 2)),\n",
    "          int(left_eye[1] + (left_eye[3] / 2)))\n",
    "        left_eye_x = left_eye_center[0]\n",
    "        left_eye_y = left_eye_center[1]\n",
    " \n",
    "        # finding rotation direction\n",
    "        if left_eye_y > right_eye_y:\n",
    "            direction = -1  \n",
    "        else:\n",
    "            direction = 1  \n",
    "       \n",
    "        delta_x = right_eye_x - left_eye_x\n",
    "        if delta_x == 0:\n",
    "            return new_img\n",
    "\n",
    "        delta_y = right_eye_y - left_eye_y\n",
    "        angle=np.arctan(delta_y/delta_x)\n",
    "        angle = (angle * 180) / np.pi\n",
    "        \n",
    "        if direction == -1:\n",
    "            angle = 90 - angle\n",
    "        else:\n",
    "            angle = -(90-angle)\n",
    " \n",
    "        new_img = Image.fromarray(img_raw)\n",
    "        new_img = np.array(new_img.rotate(direction * angle))\n",
    " \n",
    "    return new_img\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def violajones(im,minNeighbors=5):\n",
    "    img = cv2.imread(im,0)\n",
    "    faceROI = np.zeros((dim,dim), dtype=np.float32)\n",
    "    faces = face_cascade.detectMultiScale(img, minNeighbors=minNeighbors)\n",
    "    for (x, y, w, h) in faces:\n",
    "        faceROI = img[y:y + h, x:x + w]\n",
    "        faceROI = cv2.resize(faceROI,(dim,dim), interpolation=cv2.INTER_LINEAR)\n",
    "        #faceROI = preprocess(faceROI)\n",
    "        \n",
    "    return faceROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faceMean = np.zeros((dim,dim), dtype=np.float32)\n",
    "faceNumber = 0\n",
    "listOfArray = []\n",
    "faceLabel = []\n",
    "\n",
    "for key in tagFoto:\n",
    "    for im in tagFoto[key]:\n",
    "        vj = violajones(im)\n",
    "        faceMean += vj\n",
    "        flat = vj.flat\n",
    "        listOfArray.append(flat)\n",
    "        faceLabel.append(key)\n",
    "        faceNumber+=1\n",
    "\n",
    "faceMean /= faceNumber\n",
    "\n",
    "#faceMean = faceMean.astype(np.uint8)\n",
    "MatrixFlattenedImages = np.vstack(listOfArray)  \n",
    "flattenFaceMean = faceMean.flatten()\n",
    "\n",
    "fig, axes = plt.subplots(1,1,sharex=True,sharey=True,figsize=(3,3))\n",
    "axes.imshow(faceMean, cmap=\"gray\")\n",
    "axes.set_title(\"Faccia media\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decommentare solo per rifare il training del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(MatrixFlattenedImages)\n",
    "\n",
    "\n",
    "with open('pca.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(pca, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pca.pkl', 'rb') as pickle_file:\n",
    "    pca = pickle.load(pickle_file)\n",
    "\n",
    "\n",
    "varianzaCumulata = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "index95 = list(map(lambda i: i> 0.95, varianzaCumulata)).index(True)\n",
    "\n",
    "index9999 = list(map(lambda i: i> 0.9999, varianzaCumulata)).index(True)\n",
    "\n",
    "print(f\"numero di componenti per avere il 95%  \\t{index95} \\nnumero di componenti per avere il 99,99% {index9999} \")\n",
    "\n",
    "eigenfaces95perc = pca.components_[:index95]\n",
    "eigenfaces9999perc = pca.components_[:index9999]\n",
    "\n",
    "eigenfaces = pca.components_[:1190]\n",
    "\n",
    "weights = eigenfaces @ (MatrixFlattenedImages - pca.mean_).T\n",
    "weights95 = eigenfaces95perc @ (MatrixFlattenedImages - pca.mean_).T\n",
    "weights9999 = eigenfaces9999perc @ (MatrixFlattenedImages - pca.mean_).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def varyingK(k=1):\n",
    "\n",
    "    vj = violajones(\"test/0.jpg\")    \n",
    "    query = vj.reshape(1,-1)\n",
    "    query_weight = eigenfaces @ (query - pca.mean_).T\n",
    "    euclidean_distance = np.linalg.norm(weights - query_weight, axis=1)\n",
    "\n",
    "    query_weight95 = eigenfaces95perc @ (query - pca.mean_).T\n",
    "    euclidean_distance95 = np.linalg.norm(weights95 - query_weight95, axis=1)\n",
    "\n",
    "    query_weight9999 = eigenfaces9999perc @ (query - pca.mean_).T\n",
    "    euclidean_distance9999 = np.linalg.norm(weights9999 - query_weight9999, axis=1)\n",
    "\n",
    "    best_match = np.argmin(euclidean_distance)\n",
    "\n",
    "    best_match95 = np.argmin(euclidean_distance95)\n",
    "\n",
    "    best_match9999 = np.argmin(euclidean_distance9999)\n",
    "\n",
    "    fig, axes = plt.subplots(2,2,sharex=True,sharey=True,figsize=(8,6))\n",
    "    axes[0][0].imshow(query.reshape(dim,dim), cmap=\"gray\")\n",
    "    axes[0][0].set_title(f\"Query, k = {k}\")\n",
    "    axes[0][1].imshow(MatrixFlattenedImages[best_match].reshape(dim,dim), cmap=\"gray\")\n",
    "    axes[0][1].set_title(f\"{faceLabel[best_match]}\\n distanza: {euclidean_distance[best_match]}\")\n",
    "\n",
    "    axes[1][0].imshow(MatrixFlattenedImages[best_match95].reshape(dim,dim), cmap=\"gray\")\n",
    "    axes[1][0].set_title(f\"\\n95% {faceLabel[best_match95]}\\n distanza: {euclidean_distance[best_match95]}\")\n",
    "    axes[1][1].imshow(MatrixFlattenedImages[best_match9999].reshape(dim,dim), cmap=\"gray\")\n",
    "    axes[1][1].set_title(f\"\\n99% {faceLabel[best_match9999]}\\n distanza: {euclidean_distance[best_match9999]}\")\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [1,3,5]:\n",
    "    varyingK(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def violajonesMultiple(img, minNeighbors=5):\n",
    "    faceROI = np.zeros((dim,dim), dtype=np.float32)\n",
    "    faces = face_cascade.detectMultiScale(img,minNeighbors=minNeighbors, minSize = (10,10), maxSize=(50,50))\n",
    "    listOfFaceROI = []\n",
    "    pos =[]\n",
    "    for (x, y, w, h) in faces:\n",
    "        faceROI = img[y:y + h, x:x + w]\n",
    "        faceROI = cv2.resize(faceROI,(dim,dim), interpolation=cv2.INTER_LINEAR)\n",
    "        listOfFaceROI.append(faceROI)\n",
    "        pos.append([(x, y), (x+w, y+h), (255, 0, 255), 4])\n",
    "    return listOfFaceROI, pos\n",
    "\n",
    "def findCorrespondence(frame, printMatplot, printCorrespondence):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    faceNumber = 0\n",
    "    frameFaceLabel = []\n",
    "    vjMul, pos = violajonesMultiple(frame)\n",
    "    #print(f\"trovati {len(vjMul)} volti nel frame\")\n",
    "    for vj in vjMul:\n",
    "        query = vj.reshape(1,-1)\n",
    "        query_weight = eigenfaces @ (query - pca.mean_).T\n",
    "        euclidean_distance = np.linalg.norm(weights - query_weight, axis=0)\n",
    "        best_match = np.argmin(euclidean_distance)\n",
    "        frameFaceLabel.append(faceLabel[best_match])\n",
    "        if printCorrespondence:\n",
    "            print(f\"Approssimazione migliore {faceLabel[best_match]}  \\n Distanza euclidea {euclidean_distance[best_match]} \")\n",
    "        if printMatplot: \n",
    "            fig, axes = plt.subplots(1,2,sharex=True,sharey=True,figsize=(8,6))\n",
    "            axes[0].imshow(query.reshape(dim,dim), cmap=\"gray\")\n",
    "            axes[0].set_title(\"Query\")\n",
    "            axes[1].imshow(MatrixFlattenedImages[best_match].reshape(dim,dim), cmap=\"gray\")\n",
    "            axes[1].set_title(f\"{faceLabel[best_match]}\\n distanza: {euclidean_distance[best_match]}\")\n",
    "            plt.show()\n",
    "    for p in pos:\n",
    "        px,py = p[0]\n",
    "        frame = cv2.putText(frame, frameFaceLabel[faceNumber], (px-5,py-5) ,font, 1,(255,255,255),2 )\n",
    "        faceNumber+=1\n",
    "        frame = cv2.rectangle(frame, p[0],p[1],p[2],p[3])\n",
    "    #cv2.imshow(\"face detect\", frame)\n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "    return frame\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"test/0.jpg\",0)\n",
    "\n",
    "findCorrespondence(img,True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Da decommentare in caso in cui bisogna rifare la raccolta delle facce nei video o dopo aver ricreato \n",
    "#il set dell'addestramento\n",
    "\n",
    "#'''\n",
    "video = cv2.VideoCapture(\"Video finale senza riconoscimento.mp4\")\n",
    "frames = []\n",
    "if (video.isOpened()== False):\n",
    "    print(\"Error opening video file\")\n",
    "while(video.isOpened()):\n",
    "  ret, frame = video.read()\n",
    "  if ret == True:\n",
    "        frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY))\n",
    "  else:\n",
    "      break\n",
    "\n",
    "print(\"finito di raccogliere i frame\")\n",
    "riconosciuti=[]\n",
    "for f in frames:\n",
    "    riconosciuti.append(findCorrespondence(f,0,0))\n",
    "print(\"finito di riconoscere\")\n",
    "with open('Riconosciuto.pkl', 'wb') as pickle_file:\n",
    "        pickle.dump(riconosciuti, pickle_file)\n",
    "#'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#da fixare\n",
    "with open('Riconosciuto.pkl', 'rb') as pickle_file:\n",
    "    riconosciuti = pickle.load(pickle_file)\n",
    "height, width = riconosciuti[0].shape\n",
    "size = (width,height)\n",
    "\n",
    "print (len(riconosciuti))\n",
    "\n",
    "print(size)\n",
    "\n",
    "#fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "fourcc = -1 \n",
    "\n",
    "out30 = cv2.VideoWriter('project30fps.mp4',fourcc, 30, size)\n",
    "\n",
    "for i in riconosciuti:\n",
    "    out30.write(i)\n",
    "out30.release()\n",
    "\n",
    "print(\"Video 30 fps fatto\")\n",
    "\n",
    "'''\n",
    "out60 = cv2.VideoWriter('project60fps.mp4',fourcc, 60, size)\n",
    " \n",
    "for i in riconosciuti:\n",
    "    out60.write(i)\n",
    "out60.release()\n",
    "\n",
    "print(\"Video 60 fps fatto\")\n",
    "\n",
    "\n",
    "out90 = cv2.VideoWriter('project90fps.mp4',fourcc, 90, size)\n",
    " \n",
    "for i in riconosciuti:\n",
    "    out90.write(i)\n",
    "out90.release()\n",
    "\n",
    "print(\"Video 90 fps fatto\")\n",
    "'''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e9e375da22a79186ffe49072ae6df2c8be50fc2c627f3d12cad18e70942528a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
